{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERTのサンプルコード\n",
    "ディープラーニングにおける自然言語処理の王様であるBERTに関するコードを作成したので，載せてみました．大まかな流れは以下のようになっております．\n",
    "\n",
    "1. テキストデータ(ラベル付き)の読み込み\n",
    "2. 前処理(文書ごとにモデルに適した形に変換)\n",
    "3. データセット作成\n",
    "4. 2値分類用のモデルを読み込み\n",
    "5. 学習実行\n",
    "6. 精度評価\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ライブラリ作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertJapaneseTokenizer,BertForSequenceClassification, AdamW, BertConfig\n",
    "\n",
    "# 日本語の事前学習モデル\n",
    "MODEL_NAME = 'cl-tohoku/bert-base-japanese-whole-word-masking'\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データ取得\n",
    "\n",
    "Twitterのテキストを対象に，ポジティブかネガティブについて2値分類を行った．\n",
    "ポジティブ⇒0，ネガティブ⇒1\n",
    "\n",
    "今回は下記のテキストデータを対象としてみました！！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>評価</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>そこそこ美味しいけどな?そんな不味そうに見える?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>昨日コンビニでCHABAAのスイカジュース買ったん。飲んだん。くっそ不味い!!!!いや、これ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>不味っ(&amp;gt;_&amp;lt;)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>一回本場に行って不味さを体験してみたいんだがな...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>子供の頃は赤だし不味すぎぃ!って感じで初めて白味噌の味噌汁飲んだらうますぎて感動した思い出。...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>995</td>\n",
       "      <td>突発性の難聴になってしまい大量の薬を処方されたんだけど薬が不味すぎることによるストレスで完全...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>996</td>\n",
       "      <td>【食レポ5教科編】英語・・・発音アクセントが不味いが、その他は美味しい。数学・・・全体的に美...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>997</td>\n",
       "      <td>今日は雨やから買い物行くの面倒でUber使って、自分の好きな福ちゃんラーメンの炒飯頼んだんだ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>998</td>\n",
       "      <td>Twitter画像表示とかダメになってる鯖関東近郊にある会社のは色々不味そう</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>999</td>\n",
       "      <td>ロボット料理何度見ても不味そう世界3大料理台無しww</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  評価\n",
       "0                             そこそこ美味しいけどな?そんな不味そうに見える?   1\n",
       "1    昨日コンビニでCHABAAのスイカジュース買ったん。飲んだん。くっそ不味い!!!!いや、これ...   1\n",
       "2                                       不味っ(&gt;_&lt;)   0\n",
       "3                           一回本場に行って不味さを体験してみたいんだがな...   0\n",
       "4    子供の頃は赤だし不味すぎぃ!って感じで初めて白味噌の味噌汁飲んだらうますぎて感動した思い出。...   1\n",
       "..                                                 ...  ..\n",
       "995  突発性の難聴になってしまい大量の薬を処方されたんだけど薬が不味すぎることによるストレスで完全...   0\n",
       "996  【食レポ5教科編】英語・・・発音アクセントが不味いが、その他は美味しい。数学・・・全体的に美...   1\n",
       "997  今日は雨やから買い物行くの面倒でUber使って、自分の好きな福ちゃんラーメンの炒飯頼んだんだ...   1\n",
       "998             Twitter画像表示とかダメになってる鯖関東近郊にある会社のは色々不味そう   0\n",
       "999                         ロボット料理何度見ても不味そう世界3大料理台無しww   1\n",
       "\n",
       "[1000 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mazu_kanji = pd.read_json('不味.json')[[\"text\", \"評価\"]]\n",
    "df_mazu_kanji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bertに適した処理を実施"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの抽出\n",
    "sentences = df_mazu_kanji[\"text\"].values\n",
    "labels = df_mazu_kanji[\"評価\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertJapaneseTokenizer.from_pretrained(MODEL_NAME, mecab_kwargs={\"mecab_option\": mecab_path})\n",
    "\n",
    "# 元の文章\n",
    "print('original:', sentences[0])\n",
    "\n",
    "# Tokenizer：単語単位で分割された文章\n",
    "print('tokenized:', tokenizer.tokenize(sentences[0]))\n",
    "\n",
    "# Token-id：単語をIDに置き換えた文章\n",
    "print('Token IDs:', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最大単語数の確認\n",
    "max_len = []\n",
    "\n",
    "# 1文づつ処理\n",
    "for sent in sentences:\n",
    "    \n",
    "    # Tokenizeで分割\n",
    "    token_words = tokenizer.tokenize(sent)\n",
    "    \n",
    "    # 文章数を取得してリストへ格納\n",
    "    max_len.append(len(token_words))\n",
    "\n",
    "# 最大の値を確認\n",
    "print('最大単語数：', max(max_len))\n",
    "\n",
    "print('上記の最大単語数にspecial token ([CLS], [SEP])の+2をした値が最大単語数')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "# 1文づつ処理\n",
    "for sent in sentences:\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,\n",
    "                        add_special_tokens = True,       # Special Tokenの追加\n",
    "                        max_length = 107,                # 文章の長さを固定(Padding/Trancatinating)\n",
    "                        pad_to_max_length = True,        # Paddingで埋める\n",
    "                        return_attention_mask = True,    # Attention mask作成\n",
    "                        return_tensors = 'pt'            # Pytorch tensorsで返す\n",
    "                    )\n",
    "    \n",
    "    # 単語IDを取得\n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    \n",
    "    # Attention mask取得\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "    \n",
    "# リストに入ったtensorを縦方向(dim=0)へ結合\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "\n",
    "# tensor型に変換\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "# 確認\n",
    "print('original:', sentences[0])\n",
    "print('Token IDs:', input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データセットの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, random_split\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# データセットクラスの作成\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "\n",
    "# 80%地点のIDを取得\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "# データセットを分割\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print('訓練データ数：{}'.format(train_size))\n",
    "print('検証データ数：{}'.format(val_size))\n",
    "\n",
    "# データローダーの作成\n",
    "batch_size = 32\n",
    "\n",
    "# 訓練データローダー\n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,\n",
    "            sampler = RandomSampler(train_dataset), # ランダムにデータを取得してバッチ化\n",
    "            batch_size = batch_size\n",
    "        )\n",
    "\n",
    "# 検証データローダー\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset,\n",
    "            sampler = SequentialSampler(val_dataset), # 順番にデータを取得してバッチ化\n",
    "            batch_size = batch_size\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BertForSequenceClassification：学習済みモデルのロード\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,                     # モデル指定\n",
    "    num_labels = 2,                 # ラベル数：2\n",
    "    output_attentions = False,     # アテンションベクトルを出力するか\n",
    "    output_hidden_states = False,  # 隠れ層を出力するか\n",
    ")\n",
    "\n",
    "# モデルをGPUへ転送\n",
    "#model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 訓練フェーズ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最適化手法の設定\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# 訓練パートの定義\n",
    "def train(model):\n",
    "    model.train() # 訓練モードで実行\n",
    "    train_loss = 0\n",
    "    for batch in train_dataloader:\n",
    "        b_input_ids = batch[0]\n",
    "        b_input_mask = batch[1]\n",
    "        b_labels = batch[2]\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(b_input_ids,\n",
    "                             token_type_ids=None,\n",
    "                             attention_mask=b_input_mask,\n",
    "                             labels=b_labels).loss\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "    return train_loss\n",
    "\n",
    "# テストパートの定義\n",
    "def validation(model):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in validation_dataloader:\n",
    "            b_input_ids = batch[0]\n",
    "            b_input_mask = batch[1]\n",
    "            b_labels = batch[2]\n",
    "            with torch.no_grad():\n",
    "                loss = model(b_input_ids, \n",
    "                                      token_type_ids = None,\n",
    "                                      attention_mask=b_input_mask,\n",
    "                                      labels=b_labels).loss\n",
    "            val_loss += loss.item()\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習の実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習の実行\n",
    "max_epoch = 4 #エポック回数\n",
    "train_loss_ = []\n",
    "test_loss_ = []\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    train_ = train(model)\n",
    "    test_ = validation(model)\n",
    "    train_loss_.append(train_)\n",
    "    test_loss_.append(test_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 検証フェーズ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "logits_df = pd.DataFrame(np.zeros((0, 2)), columns=['logit_0', 'logit_1'])\n",
    "pred_df = pd.DataFrame(np.zeros((0, 1)), columns=['pred_label'])\n",
    "label_df = pd.DataFrame(np.zeros((0, 1)), columns=['true_label'])\n",
    "accuracy_df = pd.concat([logits_df, pred_df, label_df], axis=1)\n",
    "\n",
    "for batch in validation_dataloader:\n",
    "    b_input_ids = batch[0]\n",
    "    b_input_mask = batch[1]\n",
    "    b_labels = batch[2]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        #学習済みモデルによる予測結果をpredsで取得\n",
    "        preds = model(b_input_ids,\n",
    "                             token_type_ids=None,\n",
    "                             attention_mask=b_input_mask)\n",
    "\n",
    "    \n",
    "    temp_logits_df = pd.DataFrame(preds[0].numpy(), columns=['logit_0', 'logit_1'])\n",
    "\n",
    "    temp_pred_df = pd.DataFrame(np.argmax(preds[0].numpy(), axis=1), columns=['pred_label'])\n",
    "\n",
    "    temp_label_df = pd.DataFrame(b_labels.numpy(), columns=['true_label'])\n",
    "    \n",
    "    temp_accuracy_df = pd.concat([temp_logits_df, temp_pred_df, temp_label_df], axis=1)\n",
    "\n",
    "    accuracy_df = pd.concat([accuracy_df, temp_accuracy_df]).reset_index(drop=True)\n",
    "    \n",
    "accuracy_df[\"pred_label\"] = accuracy_df[\"pred_label\"].astype(int)\n",
    "accuracy_df[\"true_label\"] = accuracy_df[\"true_label\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>logit_0</th>\n",
       "      <th>logit_1</th>\n",
       "      <th>pred_label</th>\n",
       "      <th>true_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.464973</td>\n",
       "      <td>-1.348462</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>-3.179623</td>\n",
       "      <td>2.706445</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>-3.252740</td>\n",
       "      <td>2.803764</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>-3.022099</td>\n",
       "      <td>2.686529</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>-3.105004</td>\n",
       "      <td>2.739986</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195</td>\n",
       "      <td>-3.236476</td>\n",
       "      <td>2.715956</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>196</td>\n",
       "      <td>-3.152592</td>\n",
       "      <td>2.840008</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>197</td>\n",
       "      <td>-3.183203</td>\n",
       "      <td>2.814145</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>198</td>\n",
       "      <td>-3.227024</td>\n",
       "      <td>2.699448</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>199</td>\n",
       "      <td>-3.146456</td>\n",
       "      <td>2.660365</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      logit_0   logit_1  pred_label  true_label\n",
       "0    1.464973 -1.348462           0           0\n",
       "1   -3.179623  2.706445           1           1\n",
       "2   -3.252740  2.803764           1           1\n",
       "3   -3.022099  2.686529           1           1\n",
       "4   -3.105004  2.739986           1           1\n",
       "..        ...       ...         ...         ...\n",
       "195 -3.236476  2.715956           1           1\n",
       "196 -3.152592  2.840008           1           1\n",
       "197 -3.183203  2.814145           1           1\n",
       "198 -3.227024  2.699448           1           1\n",
       "199 -3.146456  2.660365           1           1\n",
       "\n",
       "[200 rows x 4 columns]"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 精度確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "混同行列\n",
      "[[114   4]\n",
      " [ 26  56]]\n",
      "正解率：0.85\n",
      "適合率：0.8142857142857143\n",
      "再現率：0.9661016949152542\n",
      "F1：0.8837209302325583\n"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(accuracy_df[\"true_label\"], accuracy_df[\"pred_label\"], labels=[1, 0])\n",
    "print(\"混同行列\")\n",
    "print(cm)\n",
    "\n",
    "\n",
    "#精度評価\n",
    "print('正解率：{}'.format(accuracy_score(accuracy_df[\"true_label\"], accuracy_df[\"pred_label\"])))\n",
    "print('適合率：{}'.format(precision_score(accuracy_df[\"true_label\"], accuracy_df[\"pred_label\"])))\n",
    "print('再現率：{}'.format(recall_score(accuracy_df[\"true_label\"], accuracy_df[\"pred_label\"])))\n",
    "print('F1：{}'.format(f1_score(accuracy_df[\"true_label\"], accuracy_df[\"pred_label\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデル保存\n",
    "\n",
    "学習済みのモデルを保存する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'model.pth'\n",
    "torch.save(model.to('cpu').state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
